# C-2: Automated Planning

1. Symbolic Logic and Reasoning
    - Propositional Logic
    - Truth Tables and Logical Connectives
    - First Order Logic (FOL)
    - FOL Models and Components
    - Universal and Existential Quantifiers
    - FOL Syntax Patterns
2. Introduction to Planning
    - Planning VS Execution
    - Uncertain Environments
    - Belief States
    - Vacuum Cleaner Planning Problem
    - Stochastic Environment Challenges
    - Predict-Update Cycle
3. Classical Planning
    - State Space Representation
    - Action Schemas
    - Planning Domain Definition Language (PDDL)
    - Progression Search
    - Regression Search
    - Plan Space Search
4. Additional Planning Topics
    - Situation Calculus
    - Successor-State Axioms
    - First-Order Logic Planning
    - Sliding Puzzle Example

#### Symbolic Logic and Reasoning

This course introduces knowledge-based AI agents who can reason and plan. Unlike problem-solving agents that rely solely
on algorithms like Constraint Satisfaction Problems and Search, knowledge-based agents possess knowledge about the world
and can make inferences from this knowledge to guide their actions.

##### Propositional Logic

Propositional logic is the simplest form of logical representation, consisting of symbols and logical connectives. It
forms the foundation for more complex logical systems and is crucial for representing and reasoning about knowledge in
AI.

In propositional logic, each statement (proposition) has a truth value of either true, false, or unknown. We can combine
these atomic propositions using logical connectives to form more complex expressions.

The main logical connectives in propositional logic include:

| Logical Connective | Symbol | Meaning       |
| ------------------ | ------ | ------------- |
| NOT                | ¬      | Negation      |
| AND                | ∧      | Conjunction   |
| OR                 | ∨      | Disjunction   |
| IMPLY              | ⇒      | Implication   |
| IF AND ONLY IF     | ⇔      | Biconditional |

For example, consider a simple scenario with the following atomic propositions:

- B: "There is a burglary" (true if a burglary is happening, false otherwise)
- E: "There is an earthquake" (true if an earthquake is occurring, false otherwise)
- A: "The alarm rings" (true if the alarm is triggered, false otherwise)
- M: "Mary calls" (true if Mary calls, false otherwise)
- J: "John calls" (true if John calls, false otherwise)

Using logical connectives, we can express relationships between these events:

- (E ∨ B) ⇒ A: "If there is an earthquake or a burglary, the alarm will trigger"
- A ⇒ (J ∧ M): "If the alarm triggers, both John and Mary will call"
- J ⇔ M: "John calls if and only if Mary calls"

##### Truth Tables and Logical Connectives

Truth tables help us understand how logical connectives operate by showing all possible combinations of truth values and
their results. This is essential for evaluating complex logical expressions.

For propositions P and Q:

| P     | Q     | ¬P    | P ∧ Q | P ∨ Q | P ⇒ Q | P ⇔ Q |
| ----- | ----- | ----- | ----- | ----- | ----- | ----- |
| False | False | True  | False | False | True  | True  |
| False | True  | True  | False | True  | True  | False |
| True  | False | False | False | True  | False | False |
| True  | True  | False | True  | True  | True  | True  |

Understanding this table:

1. ¬P (NOT P): True when P is False, False when P is True
2. P ∧ Q (P AND Q): True only when both P and Q are True
3. P ∨ Q (P OR Q): True when either P or Q (or both) are True
4. P ⇒ Q (P IMPLIES Q): False only when P is True and Q is False
5. P ⇔ Q (P IF AND ONLY IF Q): True when P and Q have the same truth value

Propositional statements can be categorized based on their truth values across all possible interpretations:

1. **Valid** (tautology): True in all possible models (interpretations)
    - Example: P ∨ ¬P (is always true regardless of P's value)
2. **Satisfiable**: True in at least one model
    - Example: P ∧ Q (true when both P and Q are true)
3. **Unsatisfiable** (contradiction): False in all possible models
    - Example: P ∧ ¬P (can never be true)

##### First Order Logic (FOL)

While propositional logic is useful, it has significant limitations. It can only handle boolean values and cannot
represent objects with properties or relationships between objects. First Order Logic (FOL) addresses these limitations.

FOL is built around objects and relations, with the ability to make assertions about all or some values of quantified
variables. This makes it much more expressive than propositional logic.

The key components of FOL include:

1. **Objects**: Refer to things in the world (e.g., a house, a person, an event)
2. **Relations**: Describe relationships among objects or properties of objects
3. **Functions**: Map objects to other objects
4. **Quantifiers**: Allow statements about collections of objects

<div align="center"> <img src="images/vaccum.png" width="600" height="auto"> <p style="color: #555;">Figure: Visual representation of possible states in a vacuum cleaner world</p> </div>

For example, in the statement "Peter Norvig wrote the best Artificial Intelligence textbook":

- Objects: "Peter Norvig" and "textbook"
- Properties: "the best" and "Artificial Intelligence"
- Relation: "wrote"

##### FOL Models and Components

In First Order Logic, a model represents a more complex structure than in propositional logic. While a propositional
logic model is simply a set of boolean values (e.g., {P: true, Q: false}), an FOL model consists of:

1. A domain of objects
2. Relations between these objects
3. Functions that map objects to other objects

For example, an FOL model might be represented as:

```
fol_model = { Write(Author(Peter), Book(AI textbook)) }
```

This model defines objects (Peter, AI textbook), functions (Author(), Book()), and relations (Write()).

Logical statements in FOL are built from these components:

1. **Sentences**: Express facts that can be true or false
    - Examples: Vowel(A), Above(A, B), 2 = 2
2. **Terms**: Refer to objects in the domain
    - Constants: A, B, 2
    - Variables: x, y
    - Functions: Number_of(A)
3. **Logical Connectives**: Same as in propositional logic (¬, ∧, ∨, ⇒, ⇔)
4. **Predicates**: Relations that can be true or false for various objects
    - Examples: Father(x, y), Above(x, y), Greater(x, y)

##### Universal and Existential Quantifiers

FOL introduces quantifiers that extend its expressive power by allowing statements about collections of objects:

1. **Universal Quantifier (∀)**: "For all" or "For every"
    - Example: ∀x Human(x) ⇒ Mortal(x)
    - Meaning: "All humans are mortal"
    - Often paired with implication (⇒)
2. **Existential Quantifier (∃)**: "There exists" or "For some"
    - Example: ∃x Planet(x) ∧ HasLife(x)
    - Meaning: "There exists at least one planet that has life"
    - Often paired with conjunction (∧)

These quantifiers allow FOL to express more complex ideas than propositional logic. They can also be combined:

∀x (Dog(x) ⇒ ∃y (Human(y) ∧ Owns(y, x)))

This means: "For all x, if x is a dog, then there exists a y such that y is human and y owns x" or more simply, "Every
dog has a human owner."

<div align="center"> <img src="images/belif_state_2.png" width="600" height="auto"> <p style="color: #555;">Figure: Example of partially observable environment for a vacuum cleaner agent</p> </div>

##### FOL Syntax Patterns

Certain patterns appear frequently in FOL expressions:

1. **Universal quantification** often uses implication:

    ```
    ∀x Vowel(x) ⇒ Number_of(x) = 1
    ```

    "For all x, if x is a vowel, then the number of x is 1."

2. **Existential quantification** often uses conjunction:

    ```
    ∃x Number_of(x) = 2
    ```

    "There exists an x such that the number of x is 2."

In vacuum cleaner world examples, we might represent states and actions using FOL:

```
∀d ∀l Dirt(d) ∧ Loc(l) ⇒ ¬At(d,l)
```

"For all dirt d and locations l, if d is dirt and l is a location, then d is not at l." (Initially, no dirt is at any
location)

```
∃l ∃d Dirt(d) ∧ Loc(l) ∧ At(V,l) ∧ At(d,l)
```

"There exists a location l and dirt d such that d is dirt, l is a location, the vacuum is at l, and the dirt is at l."
(There is at least one location where both the vacuum and some dirt are present)

FOL also allows us to define properties of relations:

```
∀R Transitive(R) ⇔ (∀a,b,c R(a,b) ∧ R(b,c) ⇒ R(a,c))
```

"A relation R is transitive if and only if whenever R(a,b) and R(b,c) are true, R(a,c) is also true."

The power of FOL lies in its ability to represent complex relationships and make inferences that would be impossible in
propositional logic. This makes it fundamental to knowledge representation and reasoning in AI systems that need to
understand and operate in complex environments.

By building a foundation in symbolic logic, we prepare for the next step: using this formalism to represent planning
problems that knowledge-based agents can solve.

# C-2: Automated Planning (Continued)

#### Introduction to Planning

In artificial intelligence, planning goes beyond simple search algorithms to create sequences of actions that achieve
specific goals in complex environments. While search focuses on finding paths through state spaces, planning addresses
how intelligent agents can reason about actions and their consequences to achieve desired outcomes.

##### Planning VS Execution

Planning and execution represent two distinct yet intertwined processes in intelligent agent decision-making.
Understanding the relationship between these processes is crucial for developing effective AI systems.

**Planning** involves devising a strategy or sequence of actions to achieve a specific goal, essentially mapping out a
path from the current state to a desired future state. It's a deliberative process that works with the agent's internal
model of the world.

**Execution**, on the other hand, is the implementation of these planned actions in the real environment. During
execution, the agent must adapt to actual conditions, which may differ from its internal model.

The relationship between planning and execution isn't simply sequential—it forms a continuous cycle:

1. The agent plans based on its current understanding of the world
2. It executes part of the plan
3. It observes the results of its actions
4. It updates its understanding and revises its plan if necessary

This cycle becomes particularly important in environments with uncertainty, where outcomes aren't perfectly predictable.
A good planning system must account for this uncertainty and be adaptable when things don't go exactly as expected.

<div align="center"> <img src="images/belif_state_3.png" width="600" height="auto"> <p style="color: #555;">Figure: Example of a stochastic environment where actions have probabilistic outcomes</p> </div>

##### Uncertain Environments

In the real world, agents rarely operate in perfectly predictable environments. Uncertainty can manifest in several
forms:

**Stochastic Environments**: In these settings, actions have probabilistic outcomes. Even when the same action is
performed in the same state multiple times, different results may occur. For example, in the vacuum cleaner world,
movement actions might sometimes fail due to a slippery floor.

**Multiagent Environments**: When multiple agents operate in the same environment, each agent's actions can affect
others, creating additional uncertainty. An agent cannot perfectly predict how other agents will behave, which
introduces complexity into planning.

**Partially Observable Environments**: In many realistic scenarios, agents cannot directly observe the entire state of
the environment. They must rely on sensors with limited range or accuracy, leading to incomplete information.

**Hierarchical Planning**: Some environments require planning at multiple levels of abstraction, with actions and
outcomes that aren't linearly related. This adds complexity to the planning process.

<div align="center"> <img src="images/belif_state_4.png" width="600" height="auto"> <p style="color: #555;">Figure: Planning in uncertain environments may require considering infinite sequences of actions</p> </div>

##### Belief States

In environments with uncertainty or partial observability, agents cannot always know their exact state. Instead, they
work with "belief states" – representations of the agent's knowledge and uncertainty about the actual state of the
world.

A belief state can be thought of as a set of all possible states the agent might be in, given its history of actions and
observations. As the agent gathers more information through observations, it can refine its belief state, narrowing down
the possibilities.

For example, in the sensorless vacuum cleaner problem, the agent initially doesn't know its location or the dirt
distribution. Its initial belief state includes all possible combinations of positions and dirt configurations – a total
of 8 possible states (2 positions × 2 dirt states for each position).

As the agent takes actions and makes observations, it can eliminate incompatible states from its belief set. The goal in
belief space planning is to reach a belief state where all possible states are goal states – meaning the agent has
accomplished its objective regardless of which specific state it's actually in.

<div align="center"> <img src="images/belif_state_5.png" width="600" height="auto"> <p style="color: #555;">Figure: Finding a successful plan requires navigating through belief states</p> </div>

##### Vacuum Cleaner Planning Problem

The vacuum cleaner world provides an excellent example for understanding planning in different types of environments.
Let's examine how planning varies as we introduce different levels of uncertainty:

**Fully Observable, Deterministic Environment**:

- The vacuum knows its location and can see dirt everywhere
- Actions have predictable outcomes
- Planning is straightforward: search through state space for a goal state

**Partially Observable Environment**:

- The vacuum can see its current location and whether there's dirt there
- It cannot see dirt in other locations
- Planning must account for information gathering actions

**Sensorless Environment**:

- The vacuum cannot determine its location or sense dirt
- It must plan in belief space, considering all possible states
- Actions must systematically eliminate possibilities

<div align="center"> <img src="images/vaccum_1.png" width="600" height="auto"> <p style="color: #555;">Figure: Sensorless vacuum cleaner problem with multiple possible states</p> </div>

**Stochastic Environment**:

- Movement actions might fail (e.g., the vacuum might slip and stay in place)
- Planning must account for all possible outcomes of each action
- May require conditional plans or policies rather than linear action sequences

The key insight is that as uncertainty increases, planning becomes more complex. Instead of planning a simple sequence
of actions, the agent must plan for contingencies and information gathering, working in the space of belief states
rather than world states.

<div align="center"> <img src="images/belif_state_6.png" width="600" height="auto"> <p style="color: #555;">Figure: Different planning approaches based on environment characteristics</p> </div>

##### Stochastic Environment Challenges

Planning in stochastic environments presents several unique challenges:

**Non-Deterministic Outcomes**: When actions have multiple possible outcomes, the planner must consider all possible
branches. This can lead to an exponential explosion in the number of states to consider.

**Infinite Planning Horizons**: In some stochastic environments, it's impossible to guarantee goal achievement with a
finite plan because actions might repeatedly fail. Instead, we might need to find policies that maximize the probability
of eventually reaching the goal.

**Slippery Execution**: Even a well-designed plan might fail during execution due to unexpected outcomes. Planning
systems must be able to recover and replan when things don't go as expected.

**Incomplete Knowledge**: Often, the exact probabilities of different outcomes aren't known in advance, making it
difficult to calculate optimal plans.

For example, in our vacuum cleaner world with a slippery floor, a simple plan like "Suck, Right, Suck" might not
guarantee that both squares are clean, because the "Right" action might fail, leaving the vacuum in place. Instead, the
agent might need a more complex policy that keeps trying the "Right" action until it succeeds.

<div align="center"> <img src="images/belif_state_3.png" width="600" height="auto"> <p style="color: #555;">Figure: Planning with stochastic actions requires considering multiple possible outcomes</p> </div>

##### Predict-Update Cycle

In environments with uncertainty, planning involves a continuous cycle of prediction and updating. This process allows
agents to refine their understanding of the world state as they take actions and make observations.

The cycle works as follows:

1. **Predict**: The agent uses its current belief state and planned action to predict possible new belief states. This
   prediction accounts for all possible outcomes of the action.
2. **Act**: The agent executes its chosen action in the environment.
3. **Observe**: The agent receives observations from the environment after taking the action.
4. **Update**: The agent updates its belief state based on these observations, eliminating impossible states and
   refining its knowledge.

Mathematically, this process can be represented as:

```
b' = Predict(b, a)      // Predict new belief state after action a
b1 = Update(b', o)      // Update belief based on observation o
```

Where:

- b is the current belief state
- a is the action taken
- o is the observation received
- b' is the predicted belief state after action a
- b1 is the updated belief state incorporating the observation

<div align="center"> <img src="images/belif_state_7.png" width="600" height="auto"> <p style="color: #555;">Figure: The predict-update cycle in belief state planning</p> </div>

This cycle forms the foundation of planning algorithms for partially observable and stochastic environments. By
repeatedly applying this process, agents can navigate through belief space, gradually reducing uncertainty until they
achieve their goals.

A planning graph is a specialized data structure that implements this predict-update cycle. It's organized in
alternating layers of states and actions, with links representing preconditions and effects. This structure provides
better heuristic estimates for planning, helping agents find more efficient paths through complex belief spaces.

<div align="center"> <img src="images/belif_state_8.png" width="600" height="auto"> <p style="color: #555;">Figure: Tracking the predict-update cycle in a planning problem</p> </div>

Understanding the predict-update cycle is essential for developing planning systems that can handle real-world
uncertainty. It allows agents to reason about their knowledge and uncertainties, adapting their plans as they gather
more information through interaction with the environment.

#### Classical Planning

Classical planning forms the foundation of automated planning in artificial intelligence. It provides formalized
approaches to representing problems and finding solutions in well-defined environments. While planning in uncertain
environments introduces additional complexities, understanding classical planning gives us the essential tools to build
more sophisticated planning systems.

##### State Space Representation

In classical planning, we need to clearly define the state space where our agent will operate. This representation
determines how we think about states, actions, and transitions between states.

There are two main approaches to state space representation:

**Complete Assignment** refers to a state space where every variable has a defined value. This approach works well in
deterministic, fully observable environments like those we encountered in basic search problems. In a complete
assignment, the agent knows exactly which state it's in at all times.

For example, in the vacuum cleaner world with a complete assignment, the agent would know:

- Its current location (A or B)
- Whether there's dirt in location A (yes or no)
- Whether there's dirt in location B (yes or no)

**Partial Assignment** assigns values to only some of the variables, leaving others unspecified. This is particularly
useful in stochastic or partially observable environments where the agent has incomplete information.

In a vacuum cleaner world with a partial assignment, the agent might know:

- Its current location is A
- But not know whether there's dirt in location B

**Belief State Space** generalizes these concepts by representing sets of possible states the agent might be in. A
belief state can be based on either complete or partial assignments and is essential for planning under uncertainty.

For the vacuum cleaner world, the total number of possible states is:

- 2 possible locations (A or B)
- 2 possible dirt states for each location (clean or dirty)
- Total: 2 × 2 × 2 = 8 possible states

This state space representation provides the foundation for defining planning problems and developing algorithms to
solve them.

##### Action Schemas

Action schemas describe the possible actions an agent can take in the environment. Each action schema consists of
several components that specify when the action can be applied and what happens when it is executed.

An action schema typically includes:

1. **Action Name**: A descriptive identifier for the action
2. **Parameters**: Variables that can be instantiated with specific objects
3. **Preconditions**: Logical conditions that must be true for the action to be applicable
4. **Effects**: Changes to the world state that result from executing the action

Here's an example of an action schema for a "Fly" action in an air travel domain:

```
Action(Fly(p, from, to),
  PRECOND: At(p, from) ∧ Plane(p) ∧ Airport(from) ∧ Airport(to)
  EFFECT: ¬At(p, from) ∧ At(p, to))
```

Let's break down this schema:

- **Action name**: Fly
- **Parameters**: plane (p), departure airport (from), destination airport (to)
- **Preconditions**: The plane must be at the departure airport, p must be a plane, and both locations must be airports
- **Effects**: The plane is no longer at the departure airport and is now at the destination airport

Action schemas are powerful because they provide a concise way to represent many possible actions using variables. When
these variables are instantiated with specific objects from the domain, we get concrete actions that can be applied to
specific states.

A planning agent uses these action schemas to determine which actions are possible in a given state. It checks whether
the preconditions of an action are satisfied by the current state, and if so, it can predict the resulting state by
applying the effects.

##### Planning Domain Definition Language (PDDL)

The Planning Domain Definition Language (PDDL) is a standardized language for describing planning problems. It provides
a common format that allows different planning systems to work with the same problem descriptions, facilitating research
and development in the field.

A complete PDDL description typically includes:

1. **Initial State**: The starting configuration of the world
2. **Goal Specification**: The conditions that define success
3. **Action Schemas**: The possible actions in the domain

Here's an example of a PDDL description for a simple cargo transportation problem:

```
Init(At(C1, SFO) ∧ At(C2, JFK) ∧ At(P1, SFO) ∧ At(P2, JFK)
∧ Cargo(C1) ∧ Cargo(C2) ∧ Plane(P1) ∧ Plane(P2)
∧ Airport(JFK) ∧ Airport(SFO))

Goal(At(C1, JFK) ∧ At(C2, SFO))

Action(Load(c, p, a),
  PRECOND: At(c, a) ∧ At(p, a) ∧ Cargo(c) ∧ Plane(p) ∧ Airport(a)
  EFFECT: ¬At(c, a) ∧ In(c, p))

Action(Unload(c, p, a),
  PRECOND: In(c, p) ∧ At(p, a) ∧ Cargo(c) ∧ Plane(p) ∧ Airport(a)
  EFFECT: At(c, a) ∧ ¬In(c, p))

Action(Fly(p, from, to),
  PRECOND: At(p, from) ∧ Plane(p) ∧ Airport(from) ∧ Airport(to)
  EFFECT: ¬At(p, from) ∧ At(p, to))
```

In this example:

- The initial state describes the starting locations of cargo and planes, and defines the types of objects
- The goal is to transport C1 to JFK and C2 to SFO (essentially swapping their locations)
- Three action schemas define the possible operations: Load, Unload, and Fly

To solve this problem, a planner would need to find a sequence of actions that transforms the initial state into a state
satisfying the goal conditions. One possible solution would be:

1. Load C1 onto P1 at SFO
2. Fly P1 from SFO to JFK
3. Unload C1 from P1 at JFK
4. Load C2 onto P1 at JFK
5. Fly P1 from JFK to SFO
6. Unload C2 from P1 at SFO

PDDL has become the standard representation for planning problems in research and competitions, with various extensions
to handle more complex domains including time, resources, and uncertainty.

<div align="center"> <img src="images/pacman.png" width="600" height="auto"> <p style="color: #555;">Figure: Planning problems can be applied to scenarios like navigating through a Pac-Man maze</p> </div>

##### Progression Search

Progression search, also known as forward search, is one of the primary approaches to finding solutions in the planning
problem state space. It starts from the initial state and systematically applies applicable actions to generate new
states until a goal state is reached.

The basic progression search algorithm works as follows:

1. Start with the initial state
2. Find all actions whose preconditions are satisfied in the current state
3. Apply each action to generate successor states
4. Check if any successor state satisfies the goal conditions
5. If not, continue the search from these successor states

<div align="center"> <img src="images/pacman.png" width="600" height="auto"> <p style="color: #555;">Figure: Progression search explores forward from the initial state to find a path to the goal</p> </div>

In a planning context, progression search is often visualized with the initial state on the left and the search
progressing to the right, rather than the top-to-bottom representation used in traditional tree search. This reflects
the temporal nature of planning, where we move forward in time as we execute actions.

For example, in our cargo transportation problem, progression search might start with the initial state where C1 is at
SFO and C2 is at JFK. It would consider all applicable actions in this state, such as loading C1 onto P1 at SFO or
flying P2 from JFK to SFO. For each action, it would generate a new state and continue the search.

While progression search is intuitive and widely used, it has some limitations:

1. **Irrelevant Actions**: The search may explore many actions that don't contribute to achieving the goal. For example,
   it might consider flying an empty plane between airports, which doesn't help in transporting cargo.
2. **Branching Factor**: As the number of possible actions increases, the search space grows exponentially, making
   exhaustive search impractical for large problems.

Despite these limitations, progression search forms the basis for many practical planning algorithms, often enhanced
with heuristics to guide the search toward promising directions.

<div align="center"> <img src="images/search_2.png" width="600" height="auto"> <p style="color: #555;">Figure: Different search strategies explore the state space in different patterns</p> </div>

##### Regression Search

Regression search, also known as backward search or relevant-states search, approaches the planning problem from the
opposite direction. Instead of starting from the initial state and working forward, it begins with the goal conditions
and works backward to find a path to the initial state.

The basic regression search algorithm:

1. Start with the goal conditions
2. Find all actions whose effects could contribute to achieving the current conditions
3. For each action, compute the conditions that would need to be true before applying it
4. Check if these conditions are satisfied by the initial state
5. If not, continue the search with these conditions as new subgoals

Regression search has several advantages over progression search:

1. **Reduced Branching Factor**: By focusing on actions relevant to the goal, regression search typically explores fewer
   irrelevant paths. The branching factor is often smaller because we only consider actions that can directly contribute
   to achieving our current subgoals.
2. **Goal-Directed**: The search is naturally focused on the goal, which can make it more efficient for problems where
   only a small subset of actions leads to the goal.

However, regression search also has limitations:

1. **Complex Preconditions**: When action preconditions involve complex logical expressions, computing the regression
   (the conditions needed before an action) can be more difficult.
2. **Heuristic Application**: It can be harder to apply standard heuristics in regression search, as we're working with
   partial states rather than complete states.

In practice, the choice between progression and regression search depends on the specific characteristics of the
planning problem. Some problems are more naturally solved with one approach than the other, and many advanced planning
systems incorporate elements of both.

##### Plan Space Search

Plan space search represents a different paradigm compared to progression and regression search. Instead of searching in
the state space, it searches in the space of partial plans, gradually refining an incomplete plan until it becomes a
complete solution.

A partial plan consists of:

1. A set of actions
2. Ordering constraints between actions
3. Causal links that capture dependencies between actions
4. A set of open conditions (subgoals) that still need to be achieved

The plan space search algorithm starts with a "null plan" that contains just two special actions: Start (representing
the initial state) and Finish (representing the goal conditions). All the goal conditions are initially listed as open
conditions that need to be achieved.

The search proceeds by selecting an open condition and either:

1. Using an existing action in the plan to achieve it, adding necessary ordering constraints and causal links
2. Adding a new action that can achieve it, along with appropriate constraints and links

This process continues until all open conditions are resolved, resulting in a complete plan.

<div align="center"> <img src="images/belif_state_5.png" width="600" height="auto"> <p style="color: #555;">Figure: Plan space search refines partial plans by resolving open conditions and potential conflicts</p> </div>

Plan space search has several distinctive characteristics:

1. **Least Commitment**: It delays decisions about action ordering until necessary, only adding constraints when
   required by causal relationships or to resolve conflicts.
2. **Flexibility**: The resulting plans often have partial ordering, allowing for flexible execution where actions can
   be performed in different orders as long as they satisfy the constraints.
3. **Explanatory Power**: The causal links provide a clear explanation of why each action is included in the plan and
   how actions depend on each other.

One example of plan space search is shown in the context of a sliding puzzle problem:

```
Action(Slide(t, a, b),
  Pre: On(t, a) ∧ Tile(t) ∧ Blank(b) ∧ Adj(a, b)
  Eff: On(t, b) ∧ Blank(a) ∧ ¬On(t, a) ∧ ¬Blank(b))
```

In this puzzle, we can only slide a tile to an adjacent blank space. A plan space search would build a plan by
identifying which tiles need to move where, establishing causal relationships between moves, and ensuring that conflicts
(like trying to move two tiles to the same space) are resolved.

While plan space search can be more complex to implement than state space search, it offers advantages for certain types
of problems, particularly those with complex causal dependencies or where flexible execution is important.

Classical planning techniques provide the foundation for more advanced planning approaches that can handle uncertainty,
partial observability, and real-time constraints. By understanding these fundamental concepts, we can develop planning
systems capable of solving increasingly complex real-world problems.

# C-2: Automated Planning (Continued)

#### Additional Planning Topics

As we delve deeper into automated planning, we encounter more sophisticated frameworks that build upon the foundation of
classical planning. These approaches provide greater expressiveness and flexibility, allowing us to represent and solve
increasingly complex problems.

##### Situation Calculus

Situation Calculus is a logical formalism specifically designed for reasoning about actions and change in dynamic
worlds. Developed by John McCarthy in the 1960s and refined over decades, it offers a powerful framework for
representing planning problems within first-order logic.

The key insight of Situation Calculus is treating situations—complete snapshots of the world at a particular moment—as
first-class objects that can be reasoned about. This allows us to formally represent how actions transform one situation
into another.

The fundamental components of Situation Calculus include:

1. **Situations**: Objects that represent complete states of the world at specific points in time. The initial situation
   is typically denoted as S₀.
2. **Actions**: Objects that represent things that can be done (like Fly(p,x,y) for a plane p flying from x to y).
3. **Fluents**: Relations or functions whose values can change across situations. Fluents take a situation as their last
   argument. For example, At(p,x,s) means "plane p is at location x in situation s."
4. **Result Function**: Written as Result(s,a) or sometimes do(a,s), this represents the new situation that results from
   performing action a in situation s.
5. **Possibility Axiom**: Poss(a,s) is true if action a is possible to execute in situation s.

Let's see how Situation Calculus represents a simple flying scenario:

```
Plane(p,s) ∧ Airport(x,s) ∧ Airport(y,s) ∧ At(p,x,s) ⇒ Poss(Fly(p,x,y),s)
```

This axiom states: "If p is a plane, x and y are airports, and the plane is at x in situation s, then it's possible for
the plane to fly from x to y."

The power of Situation Calculus comes from its ability to express complex dynamics within the familiar framework of
first-order logic. This means we can use standard logical inference mechanisms to reason about actions and their effects
over time.

<div align="center"> <img src="images/belif_state_6.png" width="600" height="auto"> <p style="color: #555;">Figure: Situation Calculus allows us to reason about actions and their effects across different world states</p> </div>

##### Successor-State Axioms

A key challenge in representing dynamic worlds is the frame problem—specifying what stays the same when an action
occurs. Intuitively, most properties of the world remain unchanged by most actions, but explicitly listing all these
non-effects would be impractical.

Successor-State Axioms provide an elegant solution to this problem. Rather than separately specifying what changes and
what stays the same, they offer a complete specification of when a fluent is true in the result of an action.

The general form of a successor-state axiom for a fluent F is:

```
Poss(a,s) ⇒ [F(result(a,s)) ⇔ (a made F true) ∨ (F was already true and a didn't make it false)]
```

For example, consider the successor-state axiom for the In(c,p,s) fluent, which indicates that cargo c is in plane p in
situation s:

```
Poss(a,s) ⇒ In(c,p,result(a,s)) ⇔ (a=Load(c,p,x) ∨ (In(c,p,s) ∧ a≠Unload(c,p,x)))
```

This axiom states that cargo c is in plane p after action a if and only if either:

1. Action a was loading c into p, or
2. c was already in p and action a wasn't unloading c from p

The beauty of successor-state axioms is that they simultaneously handle both effects (what changes) and frame axioms
(what stays the same) in a single, concise formula. This makes them a powerful tool for representing dynamic domains.

When combined with initial state axioms (describing S₀) and goal statements, successor-state axioms allow us to
formulate planning problems entirely within first-order logic:

```
Initial state: S₀
    At(P₁,JFK,S₀)    ∀c Cargo(c)⇒ At(c,JFK,S₀)
Goal    ∃s ∀c    Cargo(c)⇒ At(c,SFO,s)
```

This example represents a cargo transportation problem where:

- Initially, plane P₁ and all cargo are at JFK airport
- The goal is to find a situation s where all cargo is at SFO airport

Since these formulations are expressed in standard first-order logic, we can use existing theorem provers to find
solutions without needing specialized planning algorithms.

##### First-Order Logic Planning

First-Order Logic (FOL) planning combines the expressive power of FOL with planning techniques. This approach allows us
to represent and reason about complex relationships and dependencies that would be difficult to capture in simpler
formalisms.

The key advantages of FOL planning include:

1. **Expressiveness**: FOL can represent complex relationships, quantified statements, and conditional effects that
   propositional representations struggle with.
2. **Generalization**: FOL planning problems can be specified independently of the specific objects involved, allowing
   for generalized solutions.
3. **Theoretical Foundation**: The well-established semantics and inference rules of FOL provide a solid foundation for
   planning algorithms.

For example, consider a complex planning problem involving multiple agents, resources, and locations. In FOL, we can
concisely express constraints like:

```
∀a,t,l (Agent(a) ∧ Task(t) ∧ Location(l) ∧ AssignedTo(a,t) ∧ At(t,l)) ⇒ MustBeAt(a,l)
```

This axiom states that if an agent a is assigned to a task t, and task t is at location l, then agent a must be at
location l.

FOL planning approaches include:

1. **Deductive Planning**: Using logical inference to derive a plan from axioms describing the domain.
2. **Inductive Planning**: Learning planning knowledge from examples of successful plans.
3. **Hybrid Approaches**: Combining logical reasoning with heuristic search or other planning techniques.

While FOL planning is powerful, it comes with computational challenges. The expressiveness of FOL makes inference more
complex, and many reasoning tasks in FOL are theoretically undecidable. Despite these challenges, FOL planning
techniques have been successfully applied to domains requiring rich representations, such as robotic manipulation,
natural language understanding, and complex workflow management.

<div align="center"> <img src="images/search.png" width="600" height="auto"> <p style="color: #555;">Figure: First-Order Logic planning enables sophisticated reasoning about complex problem domains</p> </div>

##### Sliding Puzzle Example

The sliding puzzle provides a concrete example that demonstrates many of the planning concepts we've discussed. In a
sliding puzzle, tiles must be moved into a specific configuration by sliding them into an adjacent empty space.

Let's formalize this using an action schema:

```
Action(Slide(t, a, b),
  Pre: On(t, a) ∧ Tile(t) ∧ Blank(b) ∧ Adj(a, b)
  Eff: On(t, b) ∧ Blank(a) ∧ ¬On(t, a) ∧ ¬Blank(b))
```

Breaking down this action schema:

- **Action**: Slide(t, a, b) - Slide tile t from position a to position b

- Preconditions

    :

    - On(t, a) - Tile t is currently at position a
    - Tile(t) - t is a tile
    - Blank(b) - Position b is empty
    - Adj(a, b) - Positions a and b are adjacent

- Effects

    :

    - On(t, b) - Tile t is now at position b
    - Blank(a) - Position a is now empty
    - ¬On(t, a) - Tile t is no longer at position a
    - ¬Blank(b) - Position b is no longer empty

This formalization captures the essential constraints of the puzzle:

1. Tiles can only move to adjacent positions
2. Tiles can only move to blank positions
3. When a tile moves, it leaves behind a blank space

Solving a sliding puzzle requires finding a sequence of Slide actions that transform the initial configuration into the
goal configuration. This is a challenging planning problem because:

1. **Long Solution Sequences**: Even small puzzles can require dozens of moves to solve
2. **Large State Space**: An 8-puzzle (3×3 grid) has 9!/2 = 181,440 possible states
3. **Deadlocks**: Some configurations are unsolvable
4. **Reversibility**: Actions can be undone, potentially leading to cycles

We can address these challenges using various planning techniques:

- **Heuristic Search**: A\* search with the Manhattan distance heuristic (sum of the distances of tiles from their goal
  positions) can efficiently find optimal solutions for small puzzles.
- **Pattern Databases**: Precomputing solution costs for subproblems can create more informative heuristics for larger
  puzzles.
- **Abstraction Hierarchies**: Solving the puzzle in stages (e.g., first placing the top row, then the left column,
  etc.) can break down the problem into manageable subproblems.

The sliding puzzle example illustrates how formal planning representations can capture real-world problems and how
different planning techniques can be applied to find solutions efficiently. It also demonstrates the challenges that
planning algorithms face when dealing with complex state spaces and lengthy solution sequences.

Throughout this exploration of automated planning, we've seen how formal representations and algorithms can help
intelligent agents develop plans to achieve their goals in various environments. From basic state-space search to
sophisticated logical formalisms, planning techniques provide powerful tools for reasoning about actions and their
consequences, enabling the development of systems that can operate effectively in complex, dynamic worlds.

<div align="center"> <img src="images/belif_state_4.png" width="600" height="auto"> <p style="color: #555;">Figure: Planning approaches like those used in sliding puzzles can be applied to many sequential decision problems</p> </div>
